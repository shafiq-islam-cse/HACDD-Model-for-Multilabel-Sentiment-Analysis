{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom tqdm import tqdm, trange\nfrom sklearn.metrics import roc_auc_score\nimport pickle\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BERT_MODEL = 'bert-base-uncased'\nCASED = 'uncased' in BERT_MODEL\nINPUT = '../input/imdb/'\nTEXT_COL = 'review'\nMAXLEN = 250","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. pip install pytorch-pretrained-bert without internet","metadata":{}},{"cell_type":"code","source":"os.system('pip install --no-index --find-links=\"../input/pytorchpretrainedbert/\" pytorch_pretrained_bert')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BERT_FP = '../input/torch-bert-weights/bert-base-uncased/bert-base-uncased/'","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. create BERT model and put on GPU","metadata":{}},{"cell_type":"code","source":"def get_bert_embed_matrix():\n    bert = BertModel.from_pretrained(BERT_FP)\n    bert_embeddings = list(bert.children())[0]\n    bert_word_embeddings = list(bert_embeddings.children())[0]\n    mat = bert_word_embeddings.weight.data.numpy()\n    return mat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # tokenizer = BertTokenizer.from_pretrained(BERT_MODEL,do_lower_case = CASED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = get_bert_embed_matrix()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(INPUT + 'train_bert-base-uncased_ids.csv').sample(frac = 1.0, random_state = 23)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(INPUT + 'test_bert-base-uncased_ids.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = np.zeros((train.shape[0],MAXLEN),dtype=np.int)\n\nfor i,ids in tqdm(enumerate(list(train[TEXT_COL]))):\n\n    input_ids = [int(i) for i in ids.split()[:MAXLEN]]\n    inp_len = len(input_ids)\n    x_train[i,:inp_len] = np.array(input_ids)\n    \nx_test = np.zeros((test.shape[0],MAXLEN),dtype=np.int)\n\nfor i,ids in tqdm(enumerate(list(test[TEXT_COL]))):\n\n    input_ids = [int(i) for i in ids.split()[:MAXLEN]]\n    inp_len = len(input_ids)\n    x_test[i,:inp_len] = np.array(input_ids)\n    \nwith open('temporary.pickle', mode='wb') as f:\n    pickle.dump(x_test, f) # use temporary file to reduce memory\n\ndel x_test\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nidentity_columns = ['sentiment']\ny_identities = (train[identity_columns] >= 0.5).astype(int).values\n\n# Overall\nweights = np.ones((len(train),)) / 4\n# Subgroup\nweights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n# Background Positive, Subgroup Negative\nweights += (( (train['sentiment'].values>=0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n# Background Negative, Subgroup Positive\nweights += (( (train['sentiment'].values<0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\nloss_weight = 1.0 / weights.mean()\n\ny_train = np.vstack([(train['sentiment'].values>=0.5).astype(np.int),weights]).T\ny_aux_train = train[['sentiment]].values\n\n\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.losses import binary_crossentropy\nfrom keras import backend as K\nimport keras.layers as L\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(embedding_matrix, num_aux_targets, loss_weight):\n\n    words = Input(shape=(MAXLEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.3)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x),])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss=[custom_loss,'binary_crossentropy'], loss_weights=[loss_weight, 1.0], optimizer='adam')\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_loss(y_true, y_pred):\n    return binary_crossentropy(K.reshape(y_true[:,0],(-1,1)), y_pred) * y_true[:,1]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntr_ind, val_ind = train_test_split(list(range(len(x_train))) ,test_size = 0.05, random_state = 23)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nNUM_MODELS = 1\n\nBATCH_SIZE = 512\nEPOCHS = 5\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 512\ncheckpoint_predictions = []\ncheckpoint_val_preds = []\nweights = []\n\nfor model_idx in range(NUM_MODELS):\n    model = build_model(embedding_matrix, y_aux_train.shape[-1],loss_weight)\n    for global_epoch in range(EPOCHS):\n        model.fit(x_train[tr_ind],[y_train[tr_ind], y_aux_train[tr_ind]],validation_data = (x_train[val_ind],[y_train[val_ind], y_aux_train[val_ind]]),\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            verbose=1,\n            callbacks=[\n                LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n            ]\n        )\n        with open('temporary.pickle', mode='rb') as f:\n            x_test = pickle.load(f) # use temporary file to reduce memory\n        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n        checkpoint_val_preds.append(model.predict(x_train[val_ind], batch_size=2048)[0].flatten())\n        del x_test\n        gc.collect()\n        weights.append(2 ** global_epoch)\n    del model\n    gc.collect()\n\npredictions = np.average(checkpoint_predictions, weights=weights, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_preds = np.average(checkpoint_val_preds, weights=weights, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\ndef power_mean(x, p=-5):\n    return np.power(np.mean(np.power(x, p)),1/p)\n\ndef get_s_auc(y_true,y_pred,y_identity):\n    mask = y_identity==1\n    try:\n        s_auc = roc_auc_score(y_true[mask],y_pred[mask])\n    except:\n        s_auc = 1\n    return s_auc\n\ndef get_bpsn_auc(y_true,y_pred,y_identity):\n    mask = (y_identity==1) & (y_true==0) | (y_identity==0) & (y_true==1)\n    try:\n        bpsn_auc = roc_auc_score(y_true[mask],y_pred[mask])\n    except:\n        bpsn_auc = 1\n    return bpsn_auc\n\ndef get_bspn_auc(y_true,y_pred,y_identity):\n    mask = (y_identity==1) & (y_true==1) | (y_identity==0) & (y_true==0)\n    try:\n        bspn_auc = roc_auc_score(y_true[mask],y_pred[mask])\n    except:\n        bspn_auc = 1\n    return bspn_auc\n\ndef get_total_auc(y_true,y_pred,y_identities):\n\n    N = y_identities.shape[1]\n    \n    saucs = np.array([get_s_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n    bpsns = np.array([get_bpsn_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n    bspns = np.array([get_bspn_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n\n    M_s_auc = power_mean(saucs)\n    M_bpsns_auc = power_mean(bpsns)\n    M_bspns_auc = power_mean(bspns)\n    rauc = roc_auc_score(y_true,y_pred)\n\n\n    total_auc = M_s_auc + M_bpsns_auc + M_bspns_auc + rauc\n    total_auc/= 4\n\n    return total_auc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train[val_ind][:,0].shape, val_preds.shape,y_identities[val_ind].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_total_auc(y_train[val_ind][:,0],val_preds,y_identities[val_ind])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submit = pd.read_csv('../input//sample_submission.csv')\ndf_submit.prediction = predictions\ndf_submit.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}